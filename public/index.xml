<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DENG Lab @ SJTU</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on DENG Lab @ SJTU</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Dec 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding</title>
      <link>http://localhost:1313/blogs/lopa/</link>
      <pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/lopa/</guid>
      <description>We introduce the Lookahead Parallel Decoding (LoPA) algorithm for diffusion large language models (dLLMs) inference, enabling up to 10.1 tokens per forward pass.</description>
    </item>
    <item>
      <title>AdaMoE: Token-Adaptive Routing with Null Experts for MoE</title>
      <link>http://localhost:1313/blogs/adamoe/</link>
      <pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/adamoe/</guid>
      <description>LongCat-Flash[1] just showed a clean, large-scale deployment of token-adaptive MoE with zero-computation (identity) experts—-activating 18.6–31.3B parameters per token (~27B on average) inside a 560B MoE. Each layer mixes 512 FFN experts + 256 zero-compute experts, the router selects Top-12, and the average true-expert picks settle around ~8 thanks to a PID-style budget controller; device-level load balancing and ScMoE (shortcut-connected MoE) keep the system efficient. They present the model as a non-thinking foundation model with strong throughput/cost metrics.</description>
    </item>
    <item>
      <title>AdaMoE: 借助“空专家”实现Token级别的动态路由选择</title>
      <link>http://localhost:1313/blogs/adamoe_zh/</link>
      <pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/adamoe_zh/</guid>
      <description>LongCat-Flash[1] 近期展示了一种token级别自适应的MoE的大规模部署方案。该方案利用零计算（恒等）专家，在560B MoE模型中，每个token激活18.6B~31.3B的参数量（平均约27B）。每一层混合了512个真专家和256个零计算专家，每次选择top-12的专家，并且由于采用了PID式预算控制器，平均真专家选择数量稳定在约8个。设备级负载均衡和ScMoE保证了系统的效率。他们将该模型呈现为一个具有强大吞吐量/成本指标的非思考模型。
这种设计——添加空专家并增加top-k值，从而使每个token使用可变数量的真实专家——正是AdaMoE背后的核心思想。
太长不看版 AdaMoE 通过向专家池中添加一组 “空专家”（null experts）（其计算开销为零），为混合专家模型（MoE）带来了Token级别的自适应专家选择能力。
在 top-k路由范式下，当一个Token被路由到“空专家”时，它实际上激活了更少的真实专家。这使得每个Token所激活的真实专家数量，在保持平均计算预算不变的前提下，实现了自适应。 我们对负载均衡损失函数（load‑balancing loss）进行了微调（将所有“空专家”视为一个聚合的计算单元），并采用了一个简单的退火策略。AdaMoE在降低了计算量（FLOPs）的同时，保持甚至提升了模型的准确率。例如，在Mixtral‑8×7B模型和ARC‑C数据集上的实验显示：计算量降低了14.55%，而准确率提升了1.69%。 为何专家选择需要Token级别的自适应？ 我们将混合专家模型的路由机制从固定的 top-k 模式转向Token级别的自适应模式，其根本原因在于一个关键的观察：并非所有Token在计算上都生而平等。在一段文本中，不同Token所蕴含的信息量和处理的复杂度存在巨大差异。传统的MoE模型强制每个Token激活固定数量的专家，这种统一的计算分配方式忽略了Token间的差异，从而导致了计算资源的低效分配。 在 SocialIQA 数据集上，不同Token的累积路由概率超过50%所需的Top专家数量分布。每一条柱状图代表Mixtral-8x7B模型中，对应MoE层内不同专家数量的Token占比。 我们为此提供了实验性的证据。我们分析了Mixtral-8x7B（一个采用固定top-2路由的模型）的路由概率分布，并发现了两种关键模式：
大量Token的路由概率高度集中于单个专家，这表明激活第二个专家通常是多余的； 另有相当一部分Token的概率更均匀地分布在多个专家上，这意味着它们可能需要两个甚至更多专家的计算能力才能被有效处理。 这一发现有力地证明，固定的 top-k策略是次优的，它对简单的Token造成了过度计算，而对复杂的Token则可能计算不足。
借助“空专家”实现自适应路由 AdaMoE 通过引入 “空专家”（null experts）来实现Token级别的自适应专家选择。我们将其定义为一种不执行任何操作的单元，处理Token特征所需的计算量（FLOPs）为零。在大型语言模型的实践中，常见的零计算操作包括常数零映射和恒等映射（为简化起见，我们在后续讨论中默认采用零映射作为“空专家”的实现）。
我们的机制运行如下：
在原有的 n个真实专家之外，将专家集合扩展 m个“空专家”。 略微增加路由器的 top-k 值（例如，从 2 增加到 3 或 4）。这样，每个Token选出的 top-k专家中就可能包含一部分“空专家”。 实现计算的自适应：如果 top-k专家中包含 r个“空专家”，那么该Token实际上只使用了 k-r个真实专家。 进行合理的负载均衡：在计算负载均衡损失时，我们将所有“空专家”聚合为单一的计算单元（因为没有必要在完全等价的“空专家”之间强制实现均衡）。 在 top-k选择之后，仅对真实专家进行归一化，以确保输出的尺度与标准的MoE模型保持一致。 固定的 top-2 路由与 AdaMoE 对比。左图：标准的 top-2 路由，每个Token精确激活2个真实专家。右图：AdaMoE，从4个真实专家和5个“空专家”中选择 top-4，使得某些Token激活了3个真实专家，而另一些只激活了1个。 主要实验结果 在对 Mixtral‑8×7B 进行微调时，AdaMoE 在多个基准测试中，既降低了FFN层的计算量（FLOPs），又保持甚至提升了模型的准确率。例如，在 ARC‑Challenge 数据集上，总FLOPs下降了约14.55%，而准确率提升了1.69%。同时，每层每个Token激活的平均专家数从 2.0 降至约 1.67。 AdaMoE在多个数据集上的性能表现，展示了准确率（Acc.）、FLOPs降低百分比（%FLOPs）和平均专家负载（Load）的对比。 从论文走向实践 我们备受鼓舞地看到，“空专家”这一概念并非仅仅停留在理论层面，它已被业界前沿的大型语言模型所采纳和实现。LongCat-Flash的技术报告中，就将 “零计算专家”（zero-computation experts）列为一项关键的架构创新，并引用了我们的论文。</description>
    </item>
    <item>
      <title>SIFT: Grounding LLM Reasoning in Contexts via Stickers</title>
      <link>http://localhost:1313/blogs/sift/</link>
      <pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/sift/</guid>
      <description>Introduction &amp;amp; Motivation Techniques including Chain-of-Thought (CoT) Prompting [1,2] and Self-Consistency [3], as well as reasoning-enhanced models, e.g., OpenAI-o1, DeepSeek-R1, and KIMI-k1.5 [4-6], have all contributed to improvements in multi-step reasoning for solving hard problems.
Discussions in the community suggest that advanced reasoning capabilities in LLMs mainly stem from two factors:
foundational knowledge acquisition through massive pretraining on diverse data; strategic refinement via post-training interventions like supervised fine-tuning (SFT) or reinforcement learning (RL).</description>
    </item>
    <item>
      <title>Consistency Large Language Models: A Family of Efficient Parallel Decoders</title>
      <link>http://localhost:1313/blogs/cllm/</link>
      <pubDate>Mon, 06 May 2024 12:00:00 -0800</pubDate>
      <guid>http://localhost:1313/blogs/cllm/</guid>
      <description>TL;DR: LLMs have been traditionally regarded as sequential decoders, decoding one token after another. In this blog, we show pretrained LLMs can be easily taught to operate as efficient parallel decoders. We introduce Consistency Large Language Models (CLLMs), a new family of parallel decoders capable of reducing inference latency by efficiently decoding an $n$-token sequence per inference step. Our research shows this process &amp;ndash; mimicking human cognitive process of forming complete sentences in mind before articulating word by word &amp;ndash; can be effectively learned by simply finetuning pretrained LLMs.</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/contact/</guid>
      <description>contact</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/gpu-stats/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/gpu-stats/</guid>
      <description>GPU statistics and usage information</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/home/</guid>
      <description>home page for DENG Lab @ SJTU</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/math-examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/math-examples/</guid>
      <description>This is an inline (a^=x-b^) equation.
This is an inline $a^*=x-b^*$ equation.
These are block equations:
\[a^*=x-b^*\] \[ a^*=x-b^* \] \[ a^*=x-b^* \] These are block equations using alternate delimiters:
$$a^*=x-b^*$$ $$ a^*=x-b^* $$ $$ a^*=x-b^* $$ </description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/people/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/people/</guid>
      <description>people</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/</guid>
      <description>publications</description>
    </item>
  </channel>
</rss>
